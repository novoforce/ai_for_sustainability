{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Style transfer algorithm help in applying a style from one image to anther image\n",
    "<img src=\"../../assets/images/style_transfer.png\">\n",
    "\n",
    "What style tranfer algorithm is doing is that it is separating the content from the first image and style from the second image and merging into a third image thus getting a style transferred image.\n",
    "\n",
    "Now, When a CNN has learnt that means that the CNN layers have learnt to extract more and more complex features from the input image and here the maxpooling layers will discard **spatial information** i.e those kind of information which are irrelavent for image classification tasks.\n",
    "The effect of this is that whenever we go deeper into the CNN layers the input image is transformed into a feature map that increasingly care about the content of the image rather than any detail about the texture and the color of the pixel. Later layers of the CNN are called as the content representation of an image.\n",
    "<img src=\"../../assets/images/content_rep.png\">\n",
    "\n",
    "In this way CNN have learnt the content of the image.\n",
    "\n",
    "Now how to extract style from the image.\n",
    "Style can be thought of as traits that might be found in the brush strokes of a painting i.e its textures,color,curves and so on.\n",
    "So our task is to how to extract the style from the image.\n",
    "To extract style from the image, a feature space designed to capture texture and color information is used. This space essentially looks at spatial correlations within a layer of a network. A correlation is a measure of the relationship between 2 or more variables.\n",
    "\n",
    "<img src=\"../../assets/images/style_rep.png\">\n",
    "\n",
    "The depth represents the no of feature maps in that layer. For each feature map we can measure how strongly a feature detected **relate** to the other feature maps in that layer. for example Is a certain color detected in one map similar to a color in another map or is there any similarity between detected edges and the corners and so on..... So in nutshell the similarity and difference features in feature maps should give us some information about the texture and color information found in an image but at the same time it should leave out the actual arrangement and identity of different objects in that image.\n",
    "\n",
    "Now coming to the actual style transfer algorithm, the algo. will look at the 2 images from where content and style is to be extracted. The algo. has trained CNN which will finds style from one image and content from another image and finally merge the two images to create a third image. In this newly created image, the objects and the content is taken from the content image and colors and textures are taken from the style image\n",
    "\n",
    "<img src=\"../../assets/images/style_transfer_final.png\">\n",
    "\n",
    "#### <u> VGG19 & Content Loss </u>\n",
    "The actual style transfer paper uses the VGG19 network for the style tranfer task. An image from where content is to be extracted is given as an input to the VGG19 network and the fina layers will give **content representation** of the image.\n",
    "\n",
    "<img src=\"../../assets/images/content_represent.png\">\n",
    "\n",
    "and the next image is passed on to the network which can extract style from the image\n",
    "\n",
    "<img src=\"../../assets/images/style_represent.png\">\n",
    "\n",
    "Now after we have extracted the content and the style from the image we have to combine these to form the final image. So what's actually happening under the hood is that a copy of the content image is set as a target image and manipulation is done on the target image(content image) so that content from the target image is very close to actual content, target style very close to style of the style image.\n",
    "\n",
    "Citing the actual research paper: [Image Style Transfer Using Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf)\n",
    "\n",
    "The content representation of the image is taken as output from fourth convolutional stack(conv4_2) and made to compare with the actual content image as shown below:\n",
    "\n",
    "<img src=\"../../assets/images/content_rep1.png\">\n",
    "\n",
    "To represnet mathematically we can represent the **Content Loss** as the mean-squared error of both the losses\n",
    "\n",
    "i.e $Content_{loss} = 1/2\\sum(T_c-C_c)^2$\n",
    "\n",
    "<img src=\"../../assets/images/content_loss.png\">\n",
    "\n",
    "So while training the network our task is to minimize this loss by only changing the $T_c$ untill the target content representation matches with the content image.\n",
    "\n",
    "So we are not using VGG19 in the traditiona(classification sense) but as a feature extractor and using back propagation to minimize the defined loss function between our target and the content image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Gram Matrix</u> \n",
    "To make sure that target image has the same content as our content image we have content loss now we have to do the same for the style image.\n",
    "Style representation of the image relies on the correlations between the features in individual layers of the VGG19 network i.e looking at how similar the features in a single layer are, and now similarities include general colors and textures found in that layer. By including the correlations between multiple layers of different sizes we can obtain a multi scale representation of the input image.\n",
    "\n",
    "<img src=\"../../assets/images/multi_scale.png\">\n",
    "\n",
    "The style representation is calculated as an image passes through the convolutional layers in all 5 stacks(VGG19 Network) conv1_1,conv2_1,....conv5_1\n",
    "\n",
    "<img src=\"../../assets/images/style_conv.png\">\n",
    "\n",
    "Now the correlation between each of the layers is given by a Gram Matrix and the matrix is a result of couple of operation.\n",
    "\n",
    "Suppose we start with a (4x4) image and colvolve with 8 different filters to create a convolutional layer\n",
    "\n",
    "<img src=\"../../assets/images/gram_matrix.png\">\n",
    "\n",
    "Now this convolutional layer will have size of input image(4x4) but will have depth of 8 so the whole convolutional layer will have the shape of (4x4x8).\n",
    "Thinking about the style representation for this layer we can say that this layer has 8 feature maps that we want to find the relationship between.\n",
    "Now for that we will calculate the Gram Matrix and for this we have to vectorize the input image i.e is simplying **flattening the image**.\n",
    "\n",
    "<img src=\"../../assets/images/vectorized_image.png\">\n",
    "\n",
    "<img src=\"../../assets/images/flatten.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the vectorized feature map, for creating Gram Matrix we need to multiply the vactorized feature map with its transpose.\n",
    "\n",
    "$\\therefore$ multiplying the feature map with its transpose will get the Gram Matrix.\n",
    "This operation treats each value in the feature map as an individual sample unrelated in space to other values \n",
    "\n",
    "<img src=\"../../assets/images/gram_matrix1.png\">\n",
    "\n",
    "So the resultant matrix contain **non-localized information** about the layer.\n",
    "Non-Localized information are those which informations which will still be there even if the image was shuffled around in space.\n",
    "For example: Even if the content of a filtered image is not identifiable you should be able to see prominent colors and textures \n",
    "\n",
    "So the gram matrix will look like:\n",
    "<img src=\"../../assets/images/gram_matrix_final.png\">\n",
    "\n",
    "The values in the gram matrix will indicate the similarities between the layers for example as seen below:\n",
    "\n",
    "<img src=\"../../assets/images/similarity.png\">\n",
    "\n",
    "Note: \n",
    "Gram matrix is one of the mathematical ways of representing the idea of shared in prominent styles.\n",
    "\n",
    "Now since Gram matrix holds the style between the layers and now we can calculate the style loss for the same.\n",
    "\n",
    "#### <u>Style Loss </u>>\n",
    "For style loss we will take the mean squared distance between the style and the target image gram matrices \n",
    "\n",
    "<img src=\"../../assets/images/style_loss.png\">\n",
    "\n",
    "and all these gram matrices are calculated for each and every layers from conv1_1 to con5_1 and these lists are called $S_s T_s$\n",
    "\n",
    "<img src=\"../../assets/images/style_loss1.png\">\n",
    "\n",
    "Note: We will be only changing the target image representation $T_s$\n",
    "\n",
    "So finally we have content loss: which tells how close the content of our target image is to that of our content image and the style loss: which tells how close the style of our image is to our style image and now finally we can add these losses to get the total loss and then using backpropagation and optimization to reduce this loss in content and style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
