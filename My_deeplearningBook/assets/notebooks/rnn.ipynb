{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "So far, we've been looking at convolutional neural networks and models that allows us to analyze the spatial information in a given input image. CNN's excel in tasks that rely on finding spatial and visible patterns in training data.\n",
    "\n",
    "In this and the next couple lessons, we'll be reviewing RNN's or recurrent neural networks. These networks give us a way to incorporate memory into our neural networks, and will be critical in analyzing sequential data. RNN's are most often associated with text processing and text generation because of the way sentences are structured as a sequence of words!\n",
    "\n",
    "\n",
    "<b>The neural network architectures you've seen so far were trained using the current inputs only. We did not consider previous inputs when generating the current output. In other words, our systems did not have any memory elements. RNNs address this very basic and important issue by using memory (i.e. past inputs to the network) when producing the current output.</b>\n",
    "\n",
    "<img src=\"../images/rnn.png\">\n",
    "\n",
    "Here <b>S</b> is the previous input(memory) passing on the current input for the processing\n",
    "\n",
    "The similarity between RNN and the ANN is that we are adding memory element to the ANN and this forms the RNN as shown in the image below:\n",
    "\n",
    "<img src=\"../images/rnn_simi.png\">\n",
    "\n",
    "First approach to adding memory to the ANN\n",
    "\n",
    "<img src=\"../images/tdnn.png\">\n",
    "\n",
    "##### <u>A bit of History</u>\n",
    "The main advantage of TDNN was that the neural network was able to look beyond the current time step\n",
    "But here comes the disadvantage as well because there was a time windows to where the memory(temoral dependencies) can flow into the current input.\n",
    "\n",
    "Now there are many other networks like:\n",
    "**Elman Network** http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog1402_1/abstract which was a significant milestone in this field.\n",
    "\n",
    "But all these networks suffered from the problem so called **Vanishing gradient problem** which means that the gradient vanishes geometrically after successive iteration of gradient updates while backpropagating. So that means capturing the relationships which are 8-9 steps backward was a fundamental problem.\n",
    "Now to counter the **Vanishing gradient** problem **LSTM** and **GRU** came into existance which uses certain gates to mitigate this problem.\n",
    "\n",
    "##### <u>Recap session</u>\n",
    "\n",
    "\n",
    "###### <u>Combining CNN & RNN</u>\n",
    "\n",
    "<img src=\"../images/cnn+rnn.png\">\n",
    "\n",
    "The above system can be used for gesture recognition where CNN is used as an feature extractor and RNN for decoding the whole gesture sequence.\n",
    "\n",
    "###### <u>Feed forward neural network</u>\n",
    "\n",
    "<img src=\"../images/ffnn.png\">\n",
    "\n",
    "Here **F** is the non-linear approximation function which takes  $\\bar{x}$ and outputs $\\bar{y}$, **n** means no of inputs **k** means no of outputs $W$ is the wights associated with each inputs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take an example of single layer neural network where input layer is connected to hidden layer and the hidden layer is connected to the final layer as shown below:\n",
    "\n",
    "<img src=\"../images/ann1.png\">\n",
    "\n",
    "Here $h_i = F(x_i,{W_{ij}^1})$ that is hidden layer is related to input and the weights.\n",
    "\n",
    "$y_i = F(h_i,W_{ij}^2)$ that is final layer is related to hidden outputs and the weights.\n",
    "\n",
    "<img src=\"../images/ann_closer.png\">\n",
    "\n",
    "Now when working with neural networks there are generally 2 phases:\n",
    "\n",
    "**Training**\n",
    "\n",
    "and\n",
    "\n",
    "**Evaluation**\n",
    "\n",
    "During the training phase, we take the data set (also called the training set), which includes many pairs of inputs and their corresponding targets (outputs). Our goal is to find a set of weights that would best map the inputs to the desired outputs. In the evaluation phase, we use the network that was created in the training phase, apply our new inputs and expect to obtain the desired outputs.\n",
    "\n",
    "The training phase will include two steps:\n",
    "\n",
    "**Feedforward**\n",
    "\n",
    "and\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "We will repeat these steps as many times as we need until we decide that our system has reached the best set of weights, giving us the best possible outputs.\n",
    "\n",
    "#### <u>Mathematics(Forward Propagation)</u>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
