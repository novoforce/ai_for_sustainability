{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL101 Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this module we will be starting out with very basic mathematics and some python code to just get started with deep learning below will be few notebooks which will help in getting the basic concept right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Python basics](./assets/notebooks/0207_MorePythonBasics.ipynb)\n",
    "* [Vectors Practical](./assets/notebooks/0207_Vectors.ipynb)\n",
    "* [Object oriented python](./assets/notebooks/oops_python.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a piece of software which uses databases of expert knowledge to offer advice or make decisions in such areas as medical diagnosis.\n",
    "or simply we can say that a program which has many if else statements for a defined tasks.\n",
    "\n",
    "The expert systems have many applications like in synthesis of a drug, where after a drug is discovered and should be tested for its chemical constituents so we can encode these tests into a computer program and the computer program does the same job in the sequential order as stated by the domain expert(scientist).\n",
    "\n",
    "<img src=\"assets/images/expert_sys1.PNG\">\n",
    "<img src=\"assets/images/expert_sys2.PNG\">\n",
    "\n",
    "#### Limitations:\n",
    "\n",
    "* Lot of data which will be tedious to write an expert systems\n",
    "* The rules can be very complex\n",
    "* The rules can be sometimes be unknown like predicting a chances of patient being suffering from ebola\n",
    "\n",
    "So since these limitations of the expert systems paved a path for the era of machine learning we will be giving some examples of the data and its corresponding outcomes and make computer learn the co-relation between the data and the corresponding outputs by itself and to this we call machine learning.\n",
    "\n",
    "Now this co-relation between the data can be leaned by the some functions namely sigmoid,tan,cos,sin,exp,line,circle, $x^2$,$x^3$.....and many more functions which represents the family of functions and we are telling the computer use these functions one-by-one on the data and come up with some patterns by which we can get our job done.\n",
    "\n",
    "#### Why the ML has become successful ?\n",
    "* Availability of lot of data\n",
    "* Increasing amount of computing power at the cheaper cost\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jargon cloud\n",
    "<img src='assets/images/jargon_cloud.PNG'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 6 elements of Machine Learning\n",
    "* Data : The data can be images,text,csv,videos,logs etc\n",
    "* Task : Classification-Binary and Multi-label,Regression,Captioning,Object detection,Segmentation-Instance,semantic\n",
    "* Models: Algorithms like ANN,CNN,RNN,SVM,GANS,RandomForest,Auto-encoders etc should come up with functions which can better approximate the given problem\n",
    "* Loss: Binary,Cross-entropy,Mean-squared and the main task is to make the loss very low\n",
    "* Learning : Gradient Descent ++, Adagrad, RMSProp, Adam are some of the algorithms which help the learning(backpropagation) to happen.\n",
    "* Evaluation : We now have the trained model and we need to evaluate our model on new dataset and for that we have few metrics on which we can evaluate Precision,Recall,F1-score,Top-K\n",
    "\n",
    "We will always try to make a mapping between the 6 elements and new algorithms\n",
    "<img src='assets/images/jars.PNG'>\n",
    "\n",
    "* In most cases, our job:\n",
    "    * Data and task identification\n",
    "    * For eg: Helmet detection of motorists for identification of violations, locate number plate\n",
    "      and identify characters of the number plate.\n",
    "    * For eg: Analysing crop leaves to identify if the plant is diseased and classify which disease.\n",
    "* Mix & Match:\n",
    "    * Model, Loss, Learning, Evaluation\n",
    "    * These are pre-engineered tools, we should learn how to effectively apply them to\n",
    "        accomplish our tasks\n",
    "    * They can be tweaked to yield results in our favour, this is called Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL 102 Primitive Neurons\n",
    "From here onwards we will be looking at the building blocks of the modern deep learning techniques.The very core of the deep learning are neural networks which in turns are made up of tiny tiny connections called MP-Neuron named after Warren McCulloch and Walter Pitts in 1943 inspired by neurobiology.\n",
    "In technical sense they are also called linear threshold gate model.\n",
    "<img src='assets/images/mpneuron.PNG'>\n",
    "\n",
    "MP-Neurons are basically a **YES/NO model** bacause it can take only the boolean values as input and gives boolean values as output.\n",
    "There is only 1 parameter to be learned that is 'b' and this 'b' has some threshold value and the values determines how good the data is getting classified.\n",
    "\n",
    "So, Now referring to the 6 elements of ML we can map this model to:\n",
    "* **DATA**: Binary or boolean\n",
    "* **TASK**: Binary Classification\n",
    "* **MODEL**: A line- where trying find a equation of line(y=mx+b) here only b is the variable\n",
    "* **LOSS**: Squared loss\n",
    "* **LEARNING**: take different values of b and find which value of b gives the minimum loss\n",
    "* **EVALUATION**: At the evaluation time Accuracy is used as a metric to calculate \n",
    "\n",
    "$Accuracy = \\frac{No of correct predictions}{Total no of predictions}$\n",
    "\n",
    "#### <u>Disadvantage of the MP-Neuron:</u>\n",
    "```\n",
    "y = m*x+b\n",
    "```\n",
    "* Boolean input and Boolean outputs only\n",
    "* Linear(only Linear equation sin. cosine, functions are not possible)\n",
    "* Fixed Slope(Slope is fixed that might be -1 or so)\n",
    "* Few Intercepts (only one b (bias))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To counter the disadvantage from MP-Neuron we are introducing the preceptron model which takes the real values as opposed to boolean or yes/no values of MP-Neurons.\n",
    "because in real life the features which we gave as inputs to the AI model is more realistic rather yes/no values. Like for example take an example of a user whether she/he will buy the phone or not here we take the price,color,features such as amount of RAM,ROM,battery life, screen resolution,camera MP etc which are real numbers rather than a boolean value so thats where the need for perceptron comes into picture.\n",
    "\n",
    "#### <u>Comparison of Perceptron with the MP-Neuron</u>\n",
    "\n",
    "|****  |MP-Neuron|Perceptron|\n",
    "|------|------|------|\n",
    "| DATA | Boolean input   | Real inpput   |\n",
    "|  TASK| Boolean output| Boolean output   |\n",
    "| MODEL| Linear only one parameter 'b' to be changed($x_1+x_2 >= b$) => ($x_1+x_2-b >=0$) => ($x_2=-x_2+b$) By comapring with ($y=mx+c$) => m = -1 and c=b $\\therefore$ we can conclude **SLOPE IS CONSTANT**| weights for every input,Linear($w_1x_1+w_2x_2 = b$) => ($x_2= -\\frac{w1}{w_2}x_1+\\frac{b}{w_2}$) By comparing we get $m= -\\frac{w_1}{w_2}$, $c=\\frac{b}{w_2}$ we can conclude **SLOPE IS NOT CONSTANT**|\n",
    "|  LOSS|$\\sum_{i}(y_i - \\hat{y})^2$| $\\sum_{i}max(0,1-y_i*\\hat{y_i})$  |\n",
    "|LEARNING|Brute force| principle of trying to update weights   |\n",
    "|EVALUATION|Accuracy| Accuracy   |\n",
    "\n",
    "Model wise the perecptron looks the same as the MP-Neuron but the input values will jbe real values instead of boolean.\n",
    "<img src='assets/images/perceptron.png'>\n",
    "\n",
    "#### <u>Perceptron Learning Algorithm</u>\n",
    "<img src='assets/images/perceptron_lrn_algo.png'>\n",
    "\n",
    "##### <u>Expalanation of perceptron learning algorithm</u>\n",
    "* As we know for the perceptron the output will be binary so which means there are only 2 classes of data available for applying the perceptron learning algorithm.\n",
    "* We name the 2 classes as positive class and the negative class which are denoted by the 1 and 0 respectively.\n",
    "* Initialize the variables randomly and here the variable is 'w'\n",
    "* Till the algorithm converges\n",
    "* Randomly pick a sample($x$) and suppose if the sample is from positive class and ($w_ix_i > 0$) and if the sample is from negative class and ($w_ix_i < 0$) then there is no need to update the weights as the appropriate weights are correct so we will **continue** with the algorithm and Suppose if the sample is from a positive class but the ($w_ix_i < 0$) then that particular point(data point) is misclassified and similar case for negative class and the ($w_ix_i > 0$) and for these 2 cases we have to update the weights and the weight update is as follows:\n",
    "** If $x\\epsilon$ Positive class AND ($w_ix_i < 0$) then $w=w+x$\n",
    "** If $x\\epsilon$ Positive class AND ($w_ix_i > 0$) then $w=w-x$\n",
    "\n",
    "* And do the above update till the algorithm converges or in layman terms, for all values of x $\\epsilon$ positive class then ($w_ix_i > 0$) and if x $\\epsilon$ negative class then ($w_ix_i < 0$)\n",
    "\n",
    "**Note:**\n",
    "Perceptron learning Algorithm will only work on data which is linearly separable the perceptron\n",
    "learning algorithm will not working if data is not linearly separable\n",
    "\n",
    "* [MP Neuron & perceptron Hands-on](./assets/notebooks/mp_neuron.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL 103 Sigmoid Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sigmoid neuron is the building block the deep neural networks.\n",
    "Till as of now we are able to take the real data inputs and learn parameters $w$ and $b$ which are weights and bias and classify the data linearly.\n",
    "The limitation of the Sigmoid neuron is the model itself as the data should be linearly separable to be able to classfied by the Sigmoid neuron.\n",
    "\n",
    "The Perceptron model looks like:\n",
    "<img src='assets/images/perceptron_model.png'>\n",
    "\n",
    "As seen from the above image the perceptron model has the sharp boundaries and this why we are moving to more robust model i.e sigmoid which will make have multiple outputs based on the chances or probablities of occurring.\n",
    "\n",
    "We hereby propose the following model:\n",
    "<img src='assets/images/sigmoid.png'>\n",
    "$$y= \\frac{1}{1+e^{-(wx+b)}}$$\n",
    "The sigmoid in 'Pink' color has the smooth curve which gives the output in terms of chances of the problem occurring.\n",
    "\n",
    "#### <u>Comparison of Perceptron with the MP-Neuron</u>\n",
    "\n",
    "|****  |Perceptron|Sigmoid|\n",
    "|------|------|------|\n",
    "| DATA | Real input    | Real input   |\n",
    "|  TASK| Boolean output| Classification and regression tasks   |\n",
    "| MODEL| weights for every input,Linear($w_1x_1+w_2x_2 = b$) => ($x_2= -\\frac{w1}{w_2}x_1+\\frac{b}{w_2}$) By comparing we get $m= -\\frac{w_1}{w_2}$, $c=\\frac{b}{w_2}$ we can conclude **SLOPE IS NOT CONSTANT**| Non linear functions like **SIGMOID function** $y= \\frac{1}{1+e^(wx+b)}$|\n",
    "|  LOSS|$\\sum_{i}(y_i - \\hat{y})^2$| $\\sum_{i}(y_i - \\hat{y})^2$  |\n",
    "|LEARNING|principle of trying to update weights  | A more generic approach to the principle of trying to update weights. |\n",
    "|EVALUATION|Accuracy| Accuracy and RMSE for Regression  |\n",
    "\n",
    "For the multiple inputs the Sigmoid function can be modified to:\n",
    "$$y= \\frac{1}{1+e^{-(\\sum_i w_ix_i+b)}}\\;where\\;w = [w_1,w_2,......w_n] \\;and \\;x = [x_1,x_2,......x_n]$$\n",
    "\n",
    "A more geometric version of the above equation is:\n",
    "<!-- $$ h_ \\theta (x) =  \\frac{\\mathrm{1} }{\\mathrm{1} + e^- \\theta^Tx }  $$  -->\n",
    "\n",
    "$$y= \\frac{1}{1+e^{-(\\sum_i w_i^Tx_i+b)}}\\;where\\;w = [w_1,w_2,......w_n] \\;and \\;x = [x_1,x_2,......x_n]$$\n",
    "\n",
    "\n",
    "\n",
    "To be continued..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL 107 - Introduction to Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will look at how to use the pytorch library for creating and training the model <br>\n",
    "* [Pytorch Introduction](./assets/notebooks/understanding_pytorch.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL 108 - Convolutional Neural Network\n",
    "* [Convolutional Neural Network](./assets/notebooks/cnn_tit_bits.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL 109 - Deep Convolutional Neural Network\n",
    "The use of CNN's are in these cases as below:\n",
    "\n",
    "<img src='assets/images/cnn_tasks.png'>\n",
    "\n",
    "Some of the previous architectures for Image  based tasks which created ripples in the field of Computer vision:\n",
    "\n",
    "<img src='assets/images/benchmarks.png'>\n",
    "At the error rate of 3.5% the deep neural network algrorithms were better than humans\n",
    "\n",
    "## <b> ALEXNET </b>\n",
    "\n",
    "<img src='https://miro.medium.com/max/2812/1*bD_DMBtKwveuzIkQTwjKQQ.png'>\n",
    "\n",
    "To be continued...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "* [Autoencoder](./assets/notebooks/autoencoders.ipynb)\n",
    "\n",
    "# Style Transfer\n",
    "* [Styletransfer](./assets/notebooks/autoencoders.ipynb)\n",
    "working on style transfer\n",
    "\n",
    "# Recurrent Neural network\n",
    "* [Recurrent Neural network](./assets/notebooks/rnn.ipynb)\n",
    "working on rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
